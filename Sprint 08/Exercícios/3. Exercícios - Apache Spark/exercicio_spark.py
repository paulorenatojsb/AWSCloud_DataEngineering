# -*- coding: utf-8 -*-
"""exercicio_spark

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGCoytca4LlkM-4eKaWKreVV-IQViC6Z
"""

#Exercicio Realizado via Google Golab


# 1) Importações
from pyspark.sql import SparkSession
from pyspark import SparkContext, SQLContext
# Funções de coluna do PySpark
from pyspark.sql.functions import (
    col, rand, floor, element_at, array, lit
)

def main():
    ############################################################
    # 3.1 - ETAPA 1
    # ----------------------------------------------------------
    # Objetivo:
    # - Criar SparkSession
    # - Ler o arquivo names_aleatorios.txt
    # - Mostrar algumas linhas
    ############################################################

    # Criar a SparkSession
    spark = SparkSession \
        .builder \
        .master("local[*]") \
        .appName("Exercicio Intro") \
        .getOrCreate()

    # Caminho do arquivo (corrigido):
    caminho_arquivo = "/names_aleatorios.txt"

    # Ler CSV (sem header, sem inferSchema)
    df_nomes = spark.read.csv(
        path=caminho_arquivo,
        header=False,
        inferSchema=False
    )

    # Mostrar 5 linhas para conferir
    print("=== Etapa 1: Exibindo 5 linhas iniciais ===")
    df_nomes.show(5)

    ############################################################
    # 3.2 - ETAPA 2
    # ----------------------------------------------------------
    # Objetivo:
    # - Renomear a coluna para "Nomes"
    # - printSchema()
    # - Mostrar 10 linhas
    ############################################################
    # Se não tiver um cabeçalho, o Spark chama a coluna de "_c0"
    df_nomes = df_nomes.withColumnRenamed("_c0", "Nomes")

    print("=== Etapa 2: Schema do DataFrame após renomear coluna ===")
    df_nomes.printSchema()

    print("=== Etapa 2: Exibindo 10 linhas ===")
    df_nomes.show(10)

    ############################################################
    # 3.3 - ETAPA 3
    # ----------------------------------------------------------
    # Objetivo:
    # - Adicionar coluna "Escolaridade" com valores aleatórios
    #   entre ["Fundamental", "Medio", "Superior"]
    # - Usar métodos do Spark, evitando for/while
    ############################################################
    lista_escolaridades = array(
        lit("Fundamental"),
        lit("Medio"),
        lit("Superior")
    )

    # rand() -> gera valores [0,1). Multiplique por 3 e tire floor -> (0,1,2)
    # element_at(array, indiceBase1)
    df_nomes = df_nomes.withColumn(
    "Escolaridade",
    element_at(lista_escolaridades, (1 + floor(rand() * 3)).cast("int")) # Cast to int
)

    print("=== Etapa 3: Exibindo 5 linhas com coluna Escolaridade ===")
    df_nomes.show(5)

    ############################################################
    # 3.4 - ETAPA 4
    # ----------------------------------------------------------
    # Objetivo:
    # - Adicionar coluna "Pais" com 13 países da América do Sul
    # - Aleatoriamente, sem for/while
    ############################################################
    lista_paises = array(
        lit("Brasil"), lit("Argentina"), lit("Uruguai"), lit("Paraguai"),
        lit("Chile"), lit("Bolivia"), lit("Peru"), lit("Equador"),
        lit("Colombia"), lit("Venezuela"), lit("Guiana"), lit("Suriname"),
        lit("Guiana Francesa")
    )

    df_nomes = df_nomes.withColumn(
    "Pais",
    element_at(lista_paises, (1 + floor(rand() * 13)).cast("int")) # Cast to int
)

    print("=== Etapa 4: Exibindo 5 linhas com coluna Pais ===")
    df_nomes.show(5)

    ############################################################
    # 3.5 - ETAPA 5
    # ----------------------------------------------------------
    # Objetivo:
    # - Adicionar coluna "AnoNascimento"
    # - Valor entre 1945 e 2010, aleatório
    ############################################################
    # (2010 - 1945 + 1) = 66
    df_nomes = df_nomes.withColumn(
        "AnoNascimento",
        1945 + floor(rand() * 66)
    )

    print("=== Etapa 5: Exibindo 5 linhas com coluna AnoNascimento ===")
    df_nomes.show(5)

    ############################################################
    # 3.6 - ETAPA 6
    # ----------------------------------------------------------
    # Objetivo:
    # - Selecionar somente quem nasceu neste século (>= 2001)
    # - Salvar resultado em df_select
    # - Mostrar 10 nomes
    ############################################################
    from pyspark.sql.functions import col

    df_select = df_nomes.select("*").where(col("AnoNascimento") >= 2001)

    print("=== Etapa 6: Pessoas que nasceram neste século (10 primeiros) ===")
    df_select.show(10)

    ############################################################
    # 3.7 - ETAPA 7
    # ----------------------------------------------------------
    # Objetivo:
    # - Usar SparkSQL para fazer a mesma seleção da Etapa 6
    # - Registrar tempView e executar query
    ############################################################
    df_nomes.createOrReplaceTempView("pessoas")

    df_select_sql = spark.sql(
        "SELECT * FROM pessoas WHERE AnoNascimento >= 2001"
    )

    print("=== Etapa 7: Pessoas que nasceram neste século (via SparkSQL) ===")
    df_select_sql.show(10)

    ############################################################
    # 3.8 - ETAPA 8
    # ----------------------------------------------------------
    # Objetivo:
    # - Usar método filter para contar Millennials (1980-1994)
    ############################################################
    df_millennials = df_nomes.filter(
        (col("AnoNascimento") >= 1980) & (col("AnoNascimento") <= 1994)
    )
    qtd_millennials = df_millennials.count()

    print("=== Etapa 8: Quantidade de Millennials (DF filter) ===")
    print(qtd_millennials)

    ############################################################
    # 3.9 - ETAPA 9
    # ----------------------------------------------------------
    # Objetivo:
    # - Contar Millennials via SparkSQL
    ############################################################
    df_millennials_sql = spark.sql(
        "SELECT COUNT(*) AS QtdMillennials "
        "FROM pessoas "
        "WHERE AnoNascimento BETWEEN 1980 AND 1994"
    )

    print("=== Etapa 9: Quantidade de Millennials (SparkSQL) ===")
    df_millennials_sql.show()

    ############################################################
    # 3.10 - ETAPA 10
    # ----------------------------------------------------------
    # Objetivo:
    # - Usar Spark SQL para agrupar por Pais e geração
    # - Baby Boomers: 1944–1964
    # - Geração X: 1965–1979
    # - Millennials: 1980–1994
    # - Geração Z: 1995–2015
    ############################################################

    df_geracoes = spark.sql(
        """
        SELECT
            Pais,
            CASE
                WHEN AnoNascimento BETWEEN 1944 AND 1964 THEN 'Baby Boomers'
                WHEN AnoNascimento BETWEEN 1965 AND 1979 THEN 'Geração X'
                WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN 'Millennials'
                WHEN AnoNascimento BETWEEN 1995 AND 2015 THEN 'Geração Z'
                ELSE 'Outras Faixas'
            END AS Geracao,
            COUNT(*) AS Qtd
        FROM pessoas
        GROUP BY
            Pais,
            CASE
                WHEN AnoNascimento BETWEEN 1944 AND 1964 THEN 'Baby Boomers'
                WHEN AnoNascimento BETWEEN 1965 AND 1979 THEN 'Geração X'
                WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN 'Millennials'
                WHEN AnoNascimento BETWEEN 1995 AND 2015 THEN 'Geração Z'
                ELSE 'Outras Faixas'
            END
        ORDER BY Pais ASC, Geracao ASC, Qtd ASC
        """
    )

    print("=== Etapa 10: Pessoas por Pais e Geração ===")
    df_geracoes.show(100, truncate=False)

    ############################################################
    # Exportar o resultado final em CSV
    ############################################################
    # Define o caminho de saída (no mesmo local de names_aleatorios.txt)
    # Se names_aleatorios.txt estiver na raiz "/", o CSV também será salvo na raiz.
    # No Google Colab, normalmente o diretório é /content.
    output_path = "/names_aleatorios_resultados.csv"

    # Exporta o DataFrame para um único arquivo CSV com header, sobrescrevendo se já existir
    df_geracoes.coalesce(1).write.mode("overwrite").option("header", "true").csv(output_path)
    print(f"Arquivo CSV salvo em: {output_path}")

    ############################################################
    # Encerrar sessão Spark
    ############################################################
    spark.stop()

  # Execução principal
if __name__ == "__main__":
    main()

"""# Nova seção"""

from google.colab import drive
drive.mount('/content/drive')